{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoOnger/NIAC_Project/blob/dev/NIAC_PMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjrEkobbVNbX"
      },
      "source": [
        "# **NIAC Project: Pacman Maze Solver**\n",
        "\n",
        "*   **Authors:** Stefano Capelli, Matteo Onger\n",
        "*   **Date:** March 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yZMW6MUVWr_"
      },
      "source": [
        "**Documentation**:\n",
        "*   Gymnasium: [website](https://gymnasium.farama.org/)\n",
        "*   Pacman Maze game: [description](https://www.scallop-lang.org/ssnp24/index.html#section-17:~:text=2%3A%20PacMan%20Agent-,In,-this%20part%2C%20we)\n",
        "*   Scallop & Scallopy: [paper](https://www.researchgate.net/publication/369945806_Scallop_A_Language_for_Neurosymbolic_Programming), [repository](https://github.com/scallop-lang/scallop), [website](https://www.scallop-lang.org/)\n",
        "\n",
        "**Notes**:\n",
        "*   ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05_11SYzU9b2"
      },
      "outputs": [],
      "source": [
        "# download project repository\n",
        "!git clone -b dev https://github.com/MatteoOnger/NIAC_Project.git\n",
        "\n",
        "# download Miniconda\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-py311_25.1.1-2-Linux-x86_64.sh\n",
        "\n",
        "# download Scallopy 0.2.4 (Python 3.10)\n",
        "!wget https://github.com/scallop-lang/scallop/releases/download/0.2.4/scallopy-0.2.4-cp310-cp310-manylinux_2_27_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeWscrHaAcVj"
      },
      "outputs": [],
      "source": [
        "# install Miniconda\n",
        "%env PYTHONPATH=\n",
        "\n",
        "!chmod +x /content/Miniconda3-py311_25.1.1-2-Linux-x86_64.sh\n",
        "!bash Miniconda3-py311_25.1.1-2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.11/site-packages/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_mw-5kijoLc"
      },
      "outputs": [],
      "source": [
        "# force creation of the virtual environment 'niac'\n",
        "!conda env remove -n niac -y || true\n",
        "!conda env create -f /content/NIAC_Project/environment.yaml -y\n",
        "!conda env list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQW8ifqMZjwi"
      },
      "source": [
        "---\n",
        "## Installation check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj9kNBbcyUlM"
      },
      "outputs": [],
      "source": [
        "# ---- Hello world in Scallopy 0.2.4 ----\n",
        "%%bash\n",
        "source activate niac\n",
        "python3\n",
        "\n",
        "import sys\n",
        "print(f\"-----\\nCurrent Python version: {sys.version}\\n-----\")\n",
        "\n",
        "import scallopy\n",
        "ctx = scallopy.ScallopContext()\n",
        "\n",
        "ctx.add_relation(\"hello\", str)\n",
        "ctx.add_facts(\"hello\", [(\"Hello World\",)])\n",
        "ctx.run()\n",
        "\n",
        "print(list(ctx.relation(\"hello\")))\n",
        "quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl2e1lZqBak5"
      },
      "outputs": [],
      "source": [
        "# ---- Random agent plays pacman maze ----\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from NIAC_Project.pacman.arena import AvoidingArena\n",
        "\n",
        "env = AvoidingArena(render_mode=\"rgb_array\", grid_dim=(4,4), num_enemies=3)\n",
        "env.reset()\n",
        "\n",
        "counter = 0\n",
        "tot_reward = 0\n",
        "end_episode = False\n",
        "\n",
        "while not end_episode:\n",
        "    action = np.random.randint(0, 4)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    tot_reward += reward\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    plt.imshow(env.render())\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    time.sleep(.1)\n",
        "\n",
        "    end_episode = terminated or truncated or (counter > 10)\n",
        "    counter += 1\n",
        "\n",
        "env.close()\n",
        "print(f\"Tot reward:{tot_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6KPVQRJ8JYA"
      },
      "source": [
        "---\n",
        "## ......."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V6Q6mfViKlGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Extra"
      ],
      "metadata": {
        "id": "AnI1Ty0RnOOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Modified logic comp. to include different rewards ----\n",
        "%%bash\n",
        "source activate niac\n",
        "python3\n",
        "\n",
        "import scallopy\n",
        "import torch\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "# action space\n",
        "class ACTION(Enum):\n",
        "    RIGHT = 0\n",
        "    UP = 1\n",
        "    LEFT = 2\n",
        "    DOWN = 3\n",
        "\n",
        "# possible rewards\n",
        "rewards = {\n",
        "    \"default\": -1.0,\n",
        "    \"on_success\": 5.0,\n",
        "    \"on_failure\": -5.0,\n",
        "}\n",
        "\n",
        "# grid size\n",
        "X, Y = 3, 3\n",
        "# max number of moves (L) and min reward (R)\n",
        "L, R = 3, -100.\n",
        "\n",
        "# list of all the nodes\n",
        "nodes = [(i,j) for i in range(X) for j in range(Y)]\n",
        "\n",
        "# input data from the neural component\n",
        "agent_p = torch.zeros(X, Y)\n",
        "target_p = torch.zeros(X, Y)\n",
        "enemy_p  = torch.zeros(X, Y)\n",
        "empty_p = torch.zeros(X, Y)\n",
        "\n",
        "agent_p[1, 0] = 1.0\n",
        "target_p[2, 2] = 1.0\n",
        "enemy_p[1, 1] = 1.0\n",
        "empty_p = torch.ones(X, Y) - (target_p + enemy_p)\n",
        "\n",
        "# compute expected reward per cell\n",
        "expected_rewards = rewards[\"on_success\"] * target_p + rewards[\"on_failure\"] * enemy_p + rewards[\"default\"] * empty_p\n",
        "\n",
        "# print the expected rewards\n",
        "print(\"Grid expected rewards:\")\n",
        "print(expected_rewards, \"\\n\")\n",
        "\n",
        "\n",
        "# base context\n",
        "ctx = scallopy.ScallopContext(provenance='difftopkproofs')\n",
        "ctx.add_relation(\"actor\", (int, int))\n",
        "ctx.add_relation(\"target\", (int, int))\n",
        "ctx.add_relation(\"node\", (int, int, float))\n",
        "\n",
        "ctx.add_facts(\"actor\", [(agent_p[node], node) for node in nodes])\n",
        "ctx.add_facts(\"target\", [(target_p[node], node) for node in nodes])\n",
        "ctx.add_facts(\n",
        "    \"node\",\n",
        "    [(1.0, (node + (expected_rewards[node],))) for node in nodes],\n",
        ")\n",
        "\n",
        "ctx.add_rule(f\"edge(x, y, xp, y, {ACTION.RIGHT.value}) = node(x, y, _) and node(xp, y, _) and xp == x + 1\")\n",
        "ctx.add_rule(f\"edge(x, y, x, yp, {ACTION.UP.value})    = node(x, y, _) and node(x, yp, _) and yp == y + 1\")\n",
        "ctx.add_rule(f\"edge(x, y, xp, y, {ACTION.LEFT.value})  = node(x, y, _) and node(xp, y, _) and xp == x - 1\")\n",
        "ctx.add_rule(f\"edge(x, y, x, yp, {ACTION.DOWN.value})  = node(x, y, _) and node(x, yp, _) and yp == y - 1\")\n",
        "\n",
        "ctx.add_rule(f\"path(x, y, x, y, 1, r) = node(x, y, r)\") # path(node_i, node_j, num_moves, reward)\n",
        "ctx.add_rule(f\"path(x, y, xp, yp, 1, r) = edge(x, y, xp, yp, _) and node(xp, yp, r)\")\n",
        "ctx.add_rule(f\"path(x, y, xpp, ypp, {L}, {R}) = node(x, y, _) and node(xpp, ypp, _) and $abs(x - xpp) + $abs(y - ypp) > 1\")\n",
        "ctx.add_rule(f\"path(x, y, xpp, ypp, l + 1, rp + rn) = path(x, y, xp, yp, l, rp) and edge(xp, yp, xpp, ypp, _) and node(xpp, ypp, rn) and path(x, y, xpp, ypp, _, r) and r < rp + rn and l + 1 <= {L}\")\n",
        "\n",
        "\n",
        "# clone and run base context\n",
        "ctx1 = ctx.clone()\n",
        "ctx1.run()\n",
        "\n",
        "\n",
        "# list of all possible paths in max L moves\n",
        "paths = ctx1.relation(\"path\")\n",
        "\n",
        "# find best path between each pair of nodes\n",
        "best_paths = dict()\n",
        "for path in paths:\n",
        "    p = path[1]\n",
        "    if p[:4] not in best_paths or best_paths[p[:4]] < p[5]:\n",
        "        best_paths[p[:4]] = p[5]\n",
        "\n",
        "# print best paths\n",
        "print(f\"Best paths (node_ix, node_iy, node_jx, node_jy: reward):\\n   {best_paths}\")\n",
        "\n",
        "\n",
        "# clone and run context 2\n",
        "ctx2 = ctx.clone()\n",
        "ctx2.add_relation(\"best_path\", (int, int, int, int, float)) # best_path(node_i, node_j, reward)\n",
        "\n",
        "ctx2.add_facts(\n",
        "    \"best_path\",\n",
        "    [(1.0, k + (v,)) for k, v in best_paths.items()]\n",
        ")\n",
        "\n",
        "ctx2.add_rule(\"next_position(xp, yp, a, r) = actor(x, y) and edge(x, y, xp, yp, a) and node(xp, yp, r)\")\n",
        "ctx2.add_rule(\"next_action(a) = next_position(x, y, a, _) and best_path(x, y, gx, gy, _) and target(gx, gy)\")\n",
        "ctx2.run()\n",
        "\n",
        "\n",
        "# print final results\n",
        "print(f\"Next position (node_x, node_y, action, reward):\\n   {list(ctx2.relation('next_position'))}\")\n",
        "print(f\"Next action:\\n   {list(ctx2.relation('next_action'))}\")\n",
        "\n",
        "quit()"
      ],
      "metadata": {
        "id": "ioxEDJNUaJwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19919ea8-8453-43a9-9a38-72cfca67fa97"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid expected rewards:\n",
            "tensor([[-1., -1., -1.],\n",
            "        [-1., -5., -1.],\n",
            "        [-1., -1.,  5.]]) \n",
            "\n",
            "Best paths (node_ix, node_iy, node_jx, node_jy: reward):\n",
            "   {(0, 0, 0, 0): -1.0, (0, 0, 0, 1): -1.0, (0, 0, 0, 2): -2.0, (0, 0, 1, 0): -1.0, (0, 0, 1, 1): -6.0, (0, 0, 1, 2): -3.0, (0, 0, 2, 0): -2.0, (0, 0, 2, 1): -3.0, (0, 0, 2, 2): -100.0, (0, 1, 0, 0): -1.0, (0, 1, 0, 1): -1.0, (0, 1, 0, 2): -1.0, (0, 1, 1, 0): -2.0, (0, 1, 1, 1): -5.0, (0, 1, 1, 2): -2.0, (0, 1, 2, 0): -3.0, (0, 1, 2, 1): -6.0, (0, 1, 2, 2): 3.0, (0, 2, 0, 0): -2.0, (0, 2, 0, 1): -1.0, (0, 2, 0, 2): -1.0, (0, 2, 1, 0): -3.0, (0, 2, 1, 1): -6.0, (0, 2, 1, 2): 3.0, (0, 2, 2, 0): -100.0, (0, 2, 2, 1): 3.0, (0, 2, 2, 2): 4.0, (1, 0, 0, 0): -1.0, (1, 0, 0, 1): -2.0, (1, 0, 0, 2): -3.0, (1, 0, 1, 0): -1.0, (1, 0, 1, 1): -5.0, (1, 0, 1, 2): -6.0, (1, 0, 2, 0): -1.0, (1, 0, 2, 1): -2.0, (1, 0, 2, 2): 3.0, (1, 1, 0, 0): -2.0, (1, 1, 0, 1): -1.0, (1, 1, 0, 2): -2.0, (1, 1, 1, 0): -1.0, (1, 1, 1, 1): -5.0, (1, 1, 1, 2): 3.0, (1, 1, 2, 0): -2.0, (1, 1, 2, 1): 3.0, (1, 1, 2, 2): 4.0, (1, 2, 0, 0): -3.0, (1, 2, 0, 1): -2.0, (1, 2, 0, 2): 3.0, (1, 2, 1, 0): -6.0, (1, 2, 1, 1): -1.0, (1, 2, 1, 2): 4.0, (1, 2, 2, 0): 3.0, (1, 2, 2, 1): 4.0, (1, 2, 2, 2): 9.0, (2, 0, 0, 0): -2.0, (2, 0, 0, 1): -3.0, (2, 0, 0, 2): -100.0, (2, 0, 1, 0): -1.0, (2, 0, 1, 1): -6.0, (2, 0, 1, 2): 3.0, (2, 0, 2, 0): -1.0, (2, 0, 2, 1): 3.0, (2, 0, 2, 2): 4.0, (2, 1, 0, 0): -3.0, (2, 1, 0, 1): -6.0, (2, 1, 0, 2): 3.0, (2, 1, 1, 0): -2.0, (2, 1, 1, 1): -1.0, (2, 1, 1, 2): 4.0, (2, 1, 2, 0): 3.0, (2, 1, 2, 1): 4.0, (2, 1, 2, 2): 9.0, (2, 2, 0, 0): -100.0, (2, 2, 0, 1): -3.0, (2, 2, 0, 2): 3.0, (2, 2, 1, 0): -3.0, (2, 2, 1, 1): -1.0, (2, 2, 1, 2): 4.0, (2, 2, 2, 0): 3.0, (2, 2, 2, 1): 4.0, (2, 2, 2, 2): 9.0}\n",
            "Next position (node_x, node_y, action, reward):\n",
            "   [(tensor(1., requires_grad=True), (0, 0, 2, -1.0)), (tensor(0., requires_grad=True), (0, 0, 3, -1.0)), (tensor(0., requires_grad=True), (0, 1, 1, -1.0)), (tensor(0., requires_grad=True), (0, 1, 2, -1.0)), (tensor(0., requires_grad=True), (0, 1, 3, -1.0)), (tensor(0., requires_grad=True), (0, 2, 1, -1.0)), (tensor(0., requires_grad=True), (0, 2, 2, -1.0)), (tensor(0., requires_grad=True), (1, 0, 0, -1.0)), (tensor(0., requires_grad=True), (1, 0, 2, -1.0)), (tensor(0., requires_grad=True), (1, 0, 3, -1.0)), (tensor(0., requires_grad=True), (1, 1, 0, -5.0)), (tensor(1., requires_grad=True), (1, 1, 1, -5.0)), (tensor(0., requires_grad=True), (1, 1, 2, -5.0)), (tensor(0., requires_grad=True), (1, 1, 3, -5.0)), (tensor(0., requires_grad=True), (1, 2, 0, -1.0)), (tensor(0., requires_grad=True), (1, 2, 1, -1.0)), (tensor(0., requires_grad=True), (1, 2, 2, -1.0)), (tensor(1., requires_grad=True), (2, 0, 0, -1.0)), (tensor(0., requires_grad=True), (2, 0, 3, -1.0)), (tensor(0., requires_grad=True), (2, 1, 0, -1.0)), (tensor(0., requires_grad=True), (2, 1, 1, -1.0)), (tensor(0., requires_grad=True), (2, 1, 3, -1.0)), (tensor(0., requires_grad=True), (2, 2, 0, 5.0)), (tensor(0., requires_grad=True), (2, 2, 1, 5.0))]\n",
            "Next action:\n",
            "   [(tensor(1., requires_grad=True), (0,)), (tensor(1., requires_grad=True), (1,)), (tensor(1., requires_grad=True), (2,)), (tensor(0., requires_grad=True), (3,))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Custom provenance ----\n",
        "class MyProvenance(scallopy.provenance.ScallopProvenance):\n",
        "    def __init__(self, min :float, max :float):\n",
        "        super(MyProvenance, self).__init__()\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        return\n",
        "\n",
        "    def name(self):\n",
        "        return \"custom-provenance\"\n",
        "\n",
        "    def zero(self):\n",
        "        return self.min\n",
        "\n",
        "    def one(self):\n",
        "        return self.max\n",
        "\n",
        "    def add(self, t1, t2):\n",
        "        raise Exception(\"Not implemented\")\n",
        "\n",
        "    def mult(self, t1, t2):\n",
        "        return torch.clip(t1 + t2, min=self.min, max=self.max)\n",
        "\n",
        "    def negate(self, t):\n",
        "        return torch.clip(-t, min=self.min, max=self.max)\n",
        "\n",
        "    def saturated(self, t1, t2):\n",
        "        return bool(t1 > t2)"
      ],
      "metadata": {
        "id": "wTN1yRzUkp8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}